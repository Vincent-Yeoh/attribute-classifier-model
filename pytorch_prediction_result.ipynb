{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting modular/prediction.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile modular/prediction.py\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from pathlib import Path\n",
    "from modular import utils, model_builder\n",
    "data_dir = Path('data/16kColorMulti/')\n",
    "model_dir = Path('models')\n",
    "device = 'cpu' if torch.cuda.is_available() else 'cuda'\n",
    "\n",
    "Colors = {'white', 'black', 'green', 'blue', 'brown', 'gray', 'yellow', 'silver', 'red', 'dark', 'cream colored', 'pink', 'blond', 'orange', 'colorful', 'purple', 'maroon', 'golden', 'chrome', 'colorless',}\n",
    "Shapes =  {'rounded', 'rectangular', 'arch shaped', 'lined', 'square', 'circular', 'cylindrical',}\n",
    "Textures = {'patterned', 'striped', 'brick', 'grassy', 'fluffy', 'soft', 'pointy', 'tiled', 'floral', 'mesh', 'dotted', 'checkered', 'bushy',}\n",
    "Materials = {'wooden', 'plastic', 'steel', 'glass', 'stone', 'concrete', 'asphalt', 'leather', 'paper', 'rubber', 'fabric', 'ceramic', 'cardboard', 'clay',}\n",
    "Descriptions = {'modern', 'shiny', 'reflective', 'transparent', 'cluttered', 'bright', 'wet', 'stacked', 'folded', 'curved', 'framed', 'cushioned', 'digital', 'black lettered', 'barren', 'filled'}\n",
    "\n",
    "combined_set = set()\n",
    "\n",
    "for s in [Colors, Shapes, Textures, Materials, Descriptions]:\n",
    "    combined_set.update(s)\n",
    "classes = sorted(list(combined_set))\n",
    "  \n",
    "def get_report(label_output, conf:float, num_attribute:int):\n",
    "\n",
    "   \n",
    "\n",
    "    colors = get_category_dict(label_output=label_output, categories=Colors) \n",
    "    shapes = get_category_dict(label_output=label_output, categories=Shapes)\n",
    "    materials = get_category_dict(label_output=label_output, categories=Materials)\n",
    "    textures = get_category_dict(label_output=label_output, categories=Textures)\n",
    "    descriptions = get_category_dict(label_output=label_output, categories=Descriptions)\n",
    "\n",
    "    color_list = {key:str(value) for key, value in colors.items() if value > conf}\n",
    "    shape_list ={key:str(value) for key, value in shapes.items() if value > conf} \n",
    "    \n",
    "    materials_list ={key:str(value) for key, value in materials.items() if value > conf}\n",
    "    textures_list ={key:str(value) for key, value in textures.items() if value > conf}\n",
    "    descriptions_list =  {key:str(value) for key, value in descriptions.items() if value > conf}\n",
    "    return {'Predicted Colors': color_list, 'Predicted Shapes': shape_list, 'Predicted Material': materials_list, 'Predicted Textures': textures_list, 'Predicted Descriptions': descriptions_list}\n",
    "\n",
    "\n",
    "def get_category_dict(label_output, categories):\n",
    "    return {key:value for key, value in label_output.items() if key in categories}\n",
    "\n",
    "def predict(image_path, model_path, conf:float = 0.8, num_attribute:int = 3):\n",
    "\n",
    "    model = model_builder.EfficientNetB0V1(output_shape=len(classes), device=device)\n",
    "    model.load_state_dict(torch.load(f=model_path))\n",
    "    image = torchvision.io.read_image(str(image_path)).type(torch.float32) / 255.\n",
    "\n",
    "    transform = model.transforms\n",
    "    image = transform(image)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        image_pred = model(image.unsqueeze(dim=0).to(device))\n",
    "        image_pred = torch.sigmoid(input=image_pred)\n",
    "\n",
    "        image_pred = np.array(image_pred)\n",
    "        sorted_output = np.argsort(-image_pred)[0]\n",
    "        label_output = {classes[x]:image_pred[0][x] for x in sorted_output}\n",
    "        return get_report(label_output=label_output, conf=conf, num_attribute=num_attribute)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Colors:\n",
      "{'white': 0.79423827, 'silver': 0.77735513, 'gray': 0.7567698, 'cream colored': 0.7364935}\n",
      "Predicted Shapes:\n",
      "{'rectangular': 0.76370305, 'square': 0.5966139}\n",
      "Predicted Material:\n",
      "{'cardboard': 0.73343766, 'plastic': 0.7111691}\n",
      "Predicted Textures:\n",
      "{}\n",
      "Predicted Descriptions:\n",
      "[('digital', 0.88646626), ('cluttered', 0.6778002), ('bright', 0.62934655), ('stacked', 0.6138007), ('folded', 0.48361596)]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\60135\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "##Test\n",
    "import matplotlib.pyplot as plt\n",
    "from modular import prediction\n",
    "from pathlib import Path\n",
    "image_dir = Path('data/16kColorMulti/test/')\n",
    "image_path = image_dir / 'aircon.jpg'\n",
    "prediction.predict(image_path=image_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
