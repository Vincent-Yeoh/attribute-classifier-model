{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting modular/metrics.py\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "%%writefile modular/metrics.py\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, hamming_loss, multilabel_confusion_matrix, classification_report\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Callable, Dict\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class Result(ABC):\n",
    "    def __init__(self, loss=None):\n",
    "        self.loss = loss\n",
    "    \n",
    "    @abstractmethod\n",
    "    def __str__(self):\n",
    "        return f\"Loss: {loss:3f}\"\n",
    "    \n",
    "    \n",
    "class MultiLabelResult(Result):\n",
    "    def __init__(self, report, loss):\n",
    "        super().__init__(loss=loss)\n",
    "        self.report = report\n",
    "     \n",
    "    def __str__(self):\n",
    "\n",
    "        return f\"{self.report}\"\n",
    "\n",
    "\n",
    "def label_wise_accuracy(output, target):\n",
    "    pred = (output > 0.5).float()\n",
    "    correct = (pred == target).float()\n",
    "    label_accuracy = torch.mean(correct)\n",
    "    return label_accuracy.item()\n",
    "\n",
    "def hamming_loss_fn(y_true,y_pred):\n",
    "    y_pred, y_true = y_pred.detach().cpu(), y_true.detach().cpu()   \n",
    "    y_pred_normalized = (y_pred > 0.5).float()  \n",
    "    loss = hamming_loss(y_true=y_true, y_pred=y_pred_normalized)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def normalize_tensor(data:torch.Tensor, threshold:float):\n",
    "    data = torch.sigmoid(input=data)\n",
    "    return (data>threshold).float()\n",
    "\n",
    "def classification_report_fn(y_true, y_pred, classes):\n",
    "    y_pred = normalize_tensor(data=y_pred, threshold=0.5)\n",
    "    y_true, y_pred = y_true.detach().cpu().numpy(), y_pred.detach().cpu().numpy()\n",
    "    return classification_report(y_true=y_true,\n",
    "                      y_pred=y_pred,\n",
    "                      output_dict=True,\n",
    "                      target_names=classes,\n",
    "                      zero_division=0.0\n",
    "                      )\n",
    "                      \n",
    "def lrap_fn(y_true: torch.tensor, y_pred: torch.tensor) -> float:\n",
    "    \n",
    "    y_true, y_pred = y_true.detach().cpu().numpy(), y_pred.detach().cpu().numpy()\n",
    "    # print(f'y_true: {y_true} | y_pred : {y_pred}')\n",
    "    label_ranking_average_precision = metrics.label_ranking_average_precision_score(y_true=y_true, y_score=y_pred)\n",
    "    return label_ranking_average_precision\n",
    "\n",
    "def report_avg(dict_list: list):\n",
    "    matrix_o = np.array([[[x for x in value.values()] for key,value in batch.items()] for batch in dict_list])\n",
    "    matrix = matrix_o.mean(axis=0)\n",
    "    avg = {}\n",
    "    for i,(label, value) in list(enumerate(dict_list[0].items())):\n",
    "        avg[label] = {'precision': matrix[i][0], 'recall': matrix[i][1], 'f1-score': matrix[i][2], 'support': matrix[i][3]}\n",
    "    return avg\n",
    "\n",
    "def multiclass_accuracy_fn(y_true, y_pred_logits):\n",
    "    y_pred = y_pred_logits.argmax(dim=1)\n",
    "    return (y_pred == y_true).sum().item()/len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "multiclass-multioutput format is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\60135\\deepl\\pytorch_metrics.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/60135/deepl/pytorch_metrics.ipynb#W1sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m y_score \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([[\u001b[39m-\u001b[39m\u001b[39m0.9\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m0.008\u001b[39m, \u001b[39m0.5\u001b[39m], [\u001b[39m0.3\u001b[39m, \u001b[39m0.4\u001b[39m, \u001b[39m0.1\u001b[39m], [\u001b[39m0.1\u001b[39m,\u001b[39m0.81\u001b[39m,\u001b[39m0.89\u001b[39m], [\u001b[39m0.1\u001b[39m,\u001b[39m0.6\u001b[39m,\u001b[39m0.3\u001b[39m]])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/60135/deepl/pytorch_metrics.ipynb#W1sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# y_pred = (y_pred_logits > 0.5).float()\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/60135/deepl/pytorch_metrics.ipynb#W1sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/60135/deepl/pytorch_metrics.ipynb#W1sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m#how many top-scored-labels you have to predict in average without missing any true one\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/60135/deepl/pytorch_metrics.ipynb#W1sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# coverage_error = metrics.coverage_error(y_true=y_true,y_score=y_score)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/60135/deepl/pytorch_metrics.ipynb#W1sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/60135/deepl/pytorch_metrics.ipynb#W1sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# for each ground truth label, what fraction of higher-ranked labels were true labels?\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/60135/deepl/pytorch_metrics.ipynb#W1sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m label_ranking_average_precision \u001b[39m=\u001b[39m metrics\u001b[39m.\u001b[39;49mlrap_fn(y_true\u001b[39m=\u001b[39;49my_true, y_pred\u001b[39m=\u001b[39;49my_score)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/60135/deepl/pytorch_metrics.ipynb#W1sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m#computes the ranking loss which averages over the samples the number of label pairs that are incorrectly ordered\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/60135/deepl/pytorch_metrics.ipynb#W1sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# ranking_loss = metrics.label_ranking_loss(y_true=y_true,y_score=y_score)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/60135/deepl/pytorch_metrics.ipynb#W1sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# accuracy = metrics.accuracy_score(y_true=y_true, y_pred=y_pred)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/60135/deepl/pytorch_metrics.ipynb#W1sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# recall = metrics.recall_score(y_true=y_true, y_pred=y_pred, average='weighted')\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/60135/deepl/pytorch_metrics.ipynb#W1sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# brier_score_loss = metrics.brier_score_loss(y_true=y_true, y_prob=y_pred_logits)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/60135/deepl/pytorch_metrics.ipynb#W1sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m label_ranking_average_precision\n",
      "File \u001b[1;32mc:\\Users\\60135\\deepl\\modular\\metrics.py:56\u001b[0m, in \u001b[0;36mlrap_fn\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlrap_fn\u001b[39m(y_true: torch\u001b[39m.\u001b[39mtensor, y_pred: torch\u001b[39m.\u001b[39mtensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mfloat\u001b[39m:\n\u001b[0;32m     55\u001b[0m     y_true, y_pred \u001b[39m=\u001b[39m y_true\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy(), y_pred\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m---> 56\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39my_true: \u001b[39m\u001b[39m{\u001b[39;00my_true\u001b[39m}\u001b[39;00m\u001b[39m | y_pred : \u001b[39m\u001b[39m{\u001b[39;00my_pred\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m     57\u001b[0m     label_ranking_average_precision \u001b[39m=\u001b[39m metrics\u001b[39m.\u001b[39mlabel_ranking_average_precision_score(y_true\u001b[39m=\u001b[39my_true, y_score\u001b[39m=\u001b[39my_pred)\n\u001b[0;32m     58\u001b[0m     \u001b[39mreturn\u001b[39;00m label_ranking_average_precision\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m    207\u001b[0m         skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    209\u001b[0m         )\n\u001b[0;32m    210\u001b[0m     ):\n\u001b[1;32m--> 211\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    212\u001b[0m \u001b[39mexcept\u001b[39;00m InvalidParameterError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    213\u001b[0m     \u001b[39m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    214\u001b[0m     \u001b[39m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    215\u001b[0m     \u001b[39m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[39m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     msg \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\n\u001b[0;32m    218\u001b[0m         \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+ must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    219\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    220\u001b[0m         \u001b[39mstr\u001b[39m(e),\n\u001b[0;32m    221\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_ranking.py:1208\u001b[0m, in \u001b[0;36mlabel_ranking_average_precision_score\u001b[1;34m(y_true, y_score, sample_weight)\u001b[0m\n\u001b[0;32m   1204\u001b[0m y_type \u001b[39m=\u001b[39m type_of_target(y_true, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my_true\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1205\u001b[0m \u001b[39mif\u001b[39;00m y_type \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmultilabel-indicator\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m (\n\u001b[0;32m   1206\u001b[0m     y_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbinary\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m y_true\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m\n\u001b[0;32m   1207\u001b[0m ):\n\u001b[1;32m-> 1208\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m format is not supported\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(y_type))\n\u001b[0;32m   1210\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m issparse(y_true):\n\u001b[0;32m   1211\u001b[0m     y_true \u001b[39m=\u001b[39m csr_matrix(y_true)\n",
      "\u001b[1;31mValueError\u001b[0m: multiclass-multioutput format is not supported"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from modular import metrics\n",
    "classes = ['r','g','b']\n",
    "y_true = torch.tensor([[1, 0, 0], [0, 0, -1],[0,1,1], [0,1,0]])\n",
    "y_score = torch.tensor([[-0.9, -0.008, 0.5], [0.3, 0.4, 0.1], [0.1,0.81,0.89], [0.1,0.6,0.3]])\n",
    "# y_pred = (y_pred_logits > 0.5).float()\n",
    "\n",
    "#how many top-scored-labels you have to predict in average without missing any true one\n",
    "# coverage_error = metrics.coverage_error(y_true=y_true,y_score=y_score)\n",
    "\n",
    "# for each ground truth label, what fraction of higher-ranked labels were true labels?\n",
    "label_ranking_average_precision = metrics.lrap_fn(y_true=y_true, y_pred=y_score)\n",
    "\n",
    "#computes the ranking loss which averages over the samples the number of label pairs that are incorrectly ordered\n",
    "# ranking_loss = metrics.label_ranking_loss(y_true=y_true,y_score=y_score)\n",
    "# accuracy = metrics.accuracy_score(y_true=y_true, y_pred=y_pred)\n",
    "# auc = metrics.roc_curve(y_true=y_true, y_score=y_pred_logits)\n",
    "# precision = metrics.precision_score(y_true=y_true, y_pred=y_pred, average='weighted')\n",
    "# recall = metrics.recall_score(y_true=y_true, y_pred=y_pred, average='weighted')\n",
    "# brier_score_loss = metrics.brier_score_loss(y_true=y_true, y_prob=y_pred_logits)\n",
    "label_ranking_average_precision\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
